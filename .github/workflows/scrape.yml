# .github/workflows/scrape.yml
name: Naukri.com Scraper

on:
  schedule:
    - cron: '0 0,12 * * *' # Runs every day at midnight and noon UTC

  workflow_dispatch:
    inputs:
      invalidate_cache:
        description: 'Run with a fresh cache (ignores existing cache)'
        required: true
        type: boolean
        default: false

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
      
      # --- FINAL CACHE FIX: Use a dynamic key to force saves ---
      - name: Restore cache
        id: cache-urls
        uses: actions/cache@v4
        with:
          path: output/url_cache.json
          # Key is now unique per run, forcing a save every time.
          key: ${{ runner.os }}-urls-${{ github.run_id }}
          # Restore-keys finds the most recent cache from ANY previous run.
          restore-keys: |
            ${{ runner.os }}-urls-

      - name: Invalidate cache if requested
        if: steps.cache-urls.outputs.cache-hit == 'true' && inputs.invalidate_cache == true
        run: |
          echo "Cache invalidation was requested. Deleting restored cache file."
          rm output/url_cache.json

      - name: Install dependencies
        run: npm ci

      - name: Run the scraper
        run: node index.js
        env:
          TZ: 'Asia/Kolkata' 
          JOB_KEYWORDS: ${{ secrets.JOB_KEYWORDS }}
          JOB_LOCATION: ${{ secrets.JOB_LOCATION }}
          EXPERIENCE: ${{ secrets.EXPERIENCE }}
          PAGES_TO_SCRAPE: ${{ secrets.PAGES_TO_SCRAPE }}
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
          GCP_CLIENT_EMAIL: ${{ secrets.GCP_CLIENT_EMAIL }}
          GCP_PRIVATE_KEY: ${{ secrets.GCP_PRIVATE_KEY }}
          PROXY_LIST: ${{ secrets.PROXY_LIST }}
          HEADLESS_MODE: true

      - name: Upload output files as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-output
          path: output/